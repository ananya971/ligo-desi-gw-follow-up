{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Galaxy Selector MOC\n",
    "\n",
    "**Compared to the \"Simple Galaxy Selector, this works with Multi-Ordering-Sky-Maps**\n",
    "\n",
    "\n",
    "***NOTE (this note may be wrong, just copied from old notebook)***: Before anything can happen, download the GW event localization map. Open the terminal and type:\n",
    "\"curl -O https://gracedb.ligo.org/api/superevents/sid/files/bayestar.fits,0\"\n",
    "with sid = superevent ID (could be incorporated into this notebook)\n",
    "\n",
    "Select the galaxies to observe by WWFI in this simple manner:\n",
    "- get the LIGO event data\n",
    "- read the LIGO event data into the notebook, extract some information\n",
    "- read the DESI database into the notebook, \"clean\" the data\n",
    "- get the 90% credible regions for the event, add them as a column to the data\n",
    "- only keep data within the 90% region\n",
    "- calculate all the luminosities (& more)\n",
    "- rank them by luminosity (for now just print the TARGETID), 3D/2D localization and with a luminosity-distance 2D dependant counterpart likelihood\n",
    "\n",
    "Sources: \n",
    "- https://iopscience.iop.org/article/10.3847/0067-0049/226/1/10\n",
    "- https://emfollow.docs.ligo.org/userguide/tutorial/skymaps.html\n",
    "- https://emfollow.docs.ligo.org/userguide/tutorial/multiorder_skymaps.html\n",
    "- https://arxiv.org/pdf/1710.05452.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import all libraries, load GW data and DESI data and extract basic info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### imports, GW data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import scipy as sc\n",
    "\n",
    "from astropy.io import fits\n",
    "from astropy.table import Table, hstack\n",
    "from astropy import table\n",
    "\n",
    "import healpy as hp\n",
    "import astropy_healpix as ah\n",
    "\n",
    "from astropy.table import QTable\n",
    "\n",
    "import psycopg2\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "from astropy.cosmology import Planck18, z_at_value\n",
    "from astropy.coordinates import Distance, SkyCoord\n",
    "from astropy import units as u\n",
    "\n",
    "import astropy.constants as asc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gasel as gs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifiy the path where the data is located and then load the data (this loads everything, just have a look)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><i>QTable length=5</i>\n",
       "<table id=\"table139679465072736\" class=\"table-striped table-bordered table-condensed\">\n",
       "<thead><tr><th>UNIQ</th><th>PROBDENSITY</th><th>DISTMU</th><th>DISTSIGMA</th><th>DISTNORM</th></tr></thead>\n",
       "<thead><tr><th></th><th>1 / sr</th><th>Mpc</th><th>Mpc</th><th>1 / Mpc2</th></tr></thead>\n",
       "<thead><tr><th>int64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th></tr></thead>\n",
       "<tr><td>1024</td><td>1.5324656455152846e-49</td><td>714.1819585125542</td><td>161.80898167011566</td><td>1.8648435769602432e-06</td></tr>\n",
       "<tr><td>1025</td><td>5.385644463361342e-50</td><td>721.1167587942227</td><td>171.98003188197197</td><td>1.8195498736811365e-06</td></tr>\n",
       "<tr><td>1026</td><td>1.507013849287337e-56</td><td>512.6497643516975</td><td>199.2304182183156</td><td>3.3061639172754294e-06</td></tr>\n",
       "<tr><td>1027</td><td>9.15663190324263e-58</td><td>624.4690102361741</td><td>203.60306281109953</td><td>2.3179848195476794e-06</td></tr>\n",
       "<tr><td>1028</td><td>1.0313363324845354e-49</td><td>674.1376225795897</td><td>201.22336091582892</td><td>2.0204049691069628e-06</td></tr>\n",
       "</table></div>"
      ],
      "text/plain": [
       "<QTable length=5>\n",
       " UNIQ      PROBDENSITY       ...     DISTSIGMA             DISTNORM       \n",
       "              1 / sr         ...        Mpc                1 / Mpc2       \n",
       "int64        float64         ...      float64              float64        \n",
       "----- ---------------------- ... ------------------ ----------------------\n",
       " 1024 1.5324656455152846e-49 ... 161.80898167011566 1.8648435769602432e-06\n",
       " 1025  5.385644463361342e-50 ... 171.98003188197197 1.8195498736811365e-06\n",
       " 1026  1.507013849287337e-56 ...  199.2304182183156 3.3061639172754294e-06\n",
       " 1027   9.15663190324263e-58 ... 203.60306281109953 2.3179848195476794e-06\n",
       " 1028 1.0313363324845354e-49 ... 201.22336091582892 2.0204049691069628e-06"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"/global/homes/j/jgassert/ligo-desi-gw-follow-up/Galaxy Selection/Files/S200129mbayestar.multiorder.fits,1\"\n",
    "skymap = QTable.read(path)\n",
    "\n",
    "skymap[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get basic values from GW event\n",
    "\n",
    "We now extract some basic information  from the healpix data:\n",
    "- ipix_max: pixel with highest likelihood\n",
    "- npix: total number of pixels\n",
    "- ra, dec: ra and dec of the pixel with the highest likelihood\n",
    "\n",
    "Then we open the full fits file and extract some more basic info from the header:\n",
    "- dist_mean: mean distance of the GW event\n",
    "- dist_std: error of the distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIQ ID of highest prob sky localization:  29382844\n"
     ]
    }
   ],
   "source": [
    "# most probable sky location\n",
    "i = np.argmax(skymap[\"PROBDENSITY\"])\n",
    "uniq = skymap[i][\"UNIQ\"]\n",
    "print(\"UNIQ ID of highest prob sky localization: \", uniq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest probability at (RA, dec):  318.33984375 4.574345562095717 with prob [1/deg^2] of:  0.07437342287566126 and UNIQ ID:  29382844\n"
     ]
    }
   ],
   "source": [
    "# calculate the most probable pixel, convert to RA and dec\n",
    "level_max, ipix_max = ah.uniq_to_level_ipix(uniq)\n",
    "nside = ah.level_to_nside(level_max)\n",
    "ra, dec = ah.healpix_to_lonlat(ipix_max, nside, order = \"nested\")\n",
    "print(\"Highest probability at (RA, dec): \", ra.deg, dec.deg, \"with prob [1/deg^2] of: \", skymap[i]['PROBDENSITY'].to_value(u.deg**-2), \"and UNIQ ID: \", skymap[i][\"UNIQ\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now open the fits file and read basics info from the header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dist_mean and dist_std values:  908.4386811451134 Mpc 202.2234141038336 Mpc\n"
     ]
    }
   ],
   "source": [
    "fits_gw = fits.open(path)\n",
    "header = fits_gw[1].header\n",
    "dist_mean = header[\"DISTMEAN\"]\n",
    "dist_std = header[\"DISTSTD\"]\n",
    "print(\"The dist_mean and dist_std values: \", dist_mean*u.Mpc, dist_std*u.Mpc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these values (924, 188Mpc) are close to z= 0.19, 0.03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get DESI data\n",
    "\n",
    "Establish a connection to the DESI database and load the daily data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    db = psycopg2.connect(host='decatdb.lbl.gov', database='desidb', user='desi', password = \"5kFibers!\", port=\"5432\")\n",
    "    cursor = db.cursor()\n",
    "except (Exception, psycopg2.Error) as error:\n",
    "    print(error)\n",
    "\n",
    "cursor = db.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redux = 'daily'\n",
    "query = 'SELECT f.targetid,f.target_ra,f.target_dec,c.tileid,c.night,r.z,r.zerr,r.zwarn,r.deltachi2,f.flux_z,f.bgs_target,f.ebv, f.sersic, f.mws_target, c.filename\\n' \\\n",
    "                    f'FROM {redux}.tiles_fibermap f\\n' \\\n",
    "                    f'INNER JOIN {redux}.cumulative_tiles c ON f.cumultile_id=c.id\\n' \\\n",
    "                    f'INNER JOIN {redux}.tiles_redshifts r ON r.cumultile_id=c.id AND r.targetid=f.targetid\\n' \\\n",
    "                    f'WHERE q3c_radial_query( f.target_ra, f.target_dec, {ra.deg}, {dec.deg}, 50);'\n",
    "\n",
    "cursor.execute(query)\n",
    "rows = cursor.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Do all the calculations, selections,...\n",
    "\n",
    "- turn the daily DESI data into a Table\n",
    "- do some basic data selection stuff: only keep good + positive redshifts, only positive values of z-band flux and eliminate duplicates (right now simply takes the first entry)\n",
    "- add the ipix (healpix) pixel value for each target\n",
    "- calculate the probability for each target\n",
    "- add these values to the Table (\"PROB\"), select only targets within the 90% credible region (i.e. with a certainty of 90% the GW event is inside this region)\n",
    "- calculate the distances and its errors from the redshift, add to the data table\n",
    "- calculate the 3D probability just like here: https://iopscience.iop.org/article/10.3847/0067-0049/226/1/10 (§4) and add these values to the table\n",
    "- sort the table by its 3D probability (descending)\n",
    "- calculate absolute and apparent magnitude, luminosity and add to table\n",
    "\n",
    "The final step is to include the calculation of the most likely host galaxy from https://arxiv.org/pdf/1710.05452.pdf (GW170817); these values are then also added to the table as \"P_GAL\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create DESI data Table and data clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if rows:\n",
    "    data = Table(list(map(list, zip(*rows))),\n",
    "                             names=['TARGETID', 'TARGET_RA', 'TARGET_DEC', 'TILEID', 'NIGHT', 'Z', 'ZERR', 'ZWARN', 'DELTACHI2', 'FLUX_Z', 'BGS_TARGET', 'EBV', 'SERSIC', 'MWS_TARGET','FILENAME'])\n",
    "data[:5:-1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['ZWARN']==0]\n",
    "data = data[data['Z']>=0]\n",
    "data = data[data['FLUX_Z']>0]\n",
    "data = table.unique(data, keys = \"TARGETID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"We have {len(data)} objects in the full catalog\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Calculate the healpix id for every object in the catalog and its probability; append this data to the Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_level = 29 # highest possible HEALPix resolution that can be represented in a 64-bit signed integer\n",
    "max_nside = ah.level_to_nside(max_level)\n",
    "level, ipix = ah.uniq_to_level_ipix(skymap[\"UNIQ\"])\n",
    "\n",
    "index = ipix*(2**(max_level-level))**2\n",
    "\n",
    "sorter = np.argsort(index)\n",
    "\n",
    "# this is the NESTED pixel index of the target sky location, NOT THE UNIQ ID\n",
    "match_ipix = ah.lonlat_to_healpix(data[\"TARGET_RA\"]*u.deg, data[\"TARGET_DEC\"]*u.deg, max_nside, order='nested')\n",
    "\n",
    "# here we get the pixel index (i.e. where the entry is in the table, also NOT THE UNIQ ID) of each target\n",
    "sorter_i = sorter[np.searchsorted(index, match_ipix, side='right', sorter=sorter) - 1]\n",
    "probdensity = skymap[sorter_i]['PROBDENSITY'].to_value(u.deg**-2)\n",
    "\n",
    "# now we calculate the UNIQ ID for all our targets:\n",
    "\n",
    "uniq = skymap[\"UNIQ\"][sorter_i]\n",
    "#to make things easier in future, I will now also append the UNIQ pixels DISTMU, DISTSIGMA and DISTNORM values. However, be careful, since these DO NOT CORRESPOND THE TARGETS IN DESI DIRECTLY!\n",
    "\n",
    "distmu = skymap[sorter_i][\"DISTMU\"]\n",
    "distsigma = skymap[sorter_i][\"DISTSIGMA\"]\n",
    "distnorm = skymap[sorter_i][\"DISTNORM\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(UNIQ), len(data), len(skymap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = data.keep_columns([\"TARGETID\", \"TARGET_RA\", \"TARGET_DEC\", \"TILEID\", \"NIGHT\", \"Z\", \"ZERR\", \"ZWARN\", \"DELTACHI2\", \"FLUX_Z\", \"BGS_TARGET\", \"EBV\", \"SERSIC\", \"MWS_TARGET\", \"FILENAME\"])\n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = hstack([data, Table({\"UNIQ\": uniq}), Table({\"PROBDENSITY\": probdensity}), Table({\"DISTMU\": distmu}), Table({\"DISTSIGMA\": distsigma}), Table({\"DISTNORM\": distnorm})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The highest and lowest probability values in our catalog are: {np.min(probdensity)} and {np.max(probdensity)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Sort the skymap table and get 90% credible region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the table\n",
    "skymap_sort = Table(skymap, copy = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether copy worked:\n",
    "# skymap_sort, skymap\n",
    "# np.max(skymap[\"UNIQ\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now sort the skymap by its probability (descending)\n",
    "skymap_sort.sort('PROBDENSITY', reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this shows the first entries sorted be their probability\n",
    "skymap_sort[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now get 90% credible region area\n",
    "\n",
    "level, ipix = ah.uniq_to_level_ipix(skymap_sort['UNIQ'])\n",
    "pixel_area = ah.nside_to_pixel_area(ah.level_to_nside(level)) # this is the area each pixel contains\n",
    "\n",
    "# calculate the probability per pixel (careful: since the pixels differ in size, this is not neceseraliy a useful quantity\n",
    "prob = pixel_area * skymap_sort['PROBDENSITY']\n",
    "\n",
    "# calc cumsum of probabilities and get index of pixel that sums up to 0.9\n",
    "cumprob = np.cumsum(prob)\n",
    "i = cumprob.searchsorted(0.9)\n",
    "\n",
    "# print area\n",
    "area_90 = pixel_area[:i].sum()\n",
    "area_90.to_value(u.deg**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### can be deleted, just tried things out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skymap_sort = hstack([skymap_sort, Table({\"PROB\": prob})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skymap_sort[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skymap_prob = Table(skymap_sort, copy = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skymap_prob.sort(\"PROB\", reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skymap_prob[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compare Tables, only leave 90% credible region in DESI data table\n",
    "\n",
    "Now we can get a Table that only contains the 90% credible region. From there we only leave the matching objects in the DESI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we simply select only the entries that are within the 90% credible region\n",
    "skymap_90 = skymap_sort[:i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This is the number of UNIQ ID of objects which should lie within the 90% credible region (NOT THE NUMBER OF OBJECTS, as there can be multiple objects per UNIQ ID):\", len(np.intersect1d(skymap_90[\"UNIQ\"], data[\"UNIQ\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.in1d(data[\"UNIQ\"], skymap_90[\"UNIQ\"])\n",
    "data_90 = data[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"We now have {len(data_90)} objects in the 90% credible region\")\n",
    "print(\"Check if the region from where the galaxies have been selected makes any sense (RA; DEC): \", np.min(data_90[\"TARGET_RA\"]), np.max(data_90[\"TARGET_RA\"]), np.min(data_90[\"TARGET_DEC\"]), np.max(data_90[\"TARGET_DEC\"]))\n",
    "\n",
    "max_probdens_in_desi = np.max(data_90[\"PROBDENSITY\"])\n",
    "max_probdens_in_ligo = np.max(skymap[\"PROBDENSITY\"]).to_value(u.deg**-2)\n",
    "print(f\"We can also check whether we have at least one galaxy in the UNIQ pixel with the highest probability from LIGO...\")\n",
    "print(f\"This is {np.allclose(max_probdens_in_desi, max_probdens_in_ligo)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_90.sort('PROBDENSITY', reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_90[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"We have now performed all the necessary steps to get the localization selection in 2D, i.e. the angle on the sky\")\n",
    "min_z = np.min(data_90[\"Z\"])\n",
    "max_z = np.max(data_90[\"Z\"])\n",
    "print(f\"However, we have not used the distance data at all. Therefore our redhsifts range from {min_z} to {max_z}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate all the distances of the objects from the redshifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = Distance(z=data_90[\"Z\"], cosmology=Planck18)\n",
    "distances_err = Distance(z=data_90[\"ZERR\"], cosmology=Planck18)\n",
    "\n",
    "data_90 = hstack([data_90, Table({\"DIST_Z\": distances}), Table({\"DIST_Z_ERR\": distances_err})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_90[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets check how much the distance measurement changes in the 90% credible region of LIGO and compare to our DESI data\n",
    "\n",
    "We should see a lot more data in DESI (since we haven't done any redshift selection and there can be multiple objects per GW-pixel). Be carfeul, we are comparing actual object counts with the number of pixels.\n",
    "\n",
    "\n",
    "We still see, that most of the GW localization is roughly between 780Mpc and 950Mpc. DESI definitely has a most of its data in this range as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_max_90 = np.max(skymap_90[\"DISTMU\"])*u.Mpc\n",
    "dist_min_90 = np.min(skymap_90[\"DISTMU\"])*u.Mpc\n",
    "print(f\"Our distance ranges from {dist_min_90} to {dist_max_90}\")\n",
    "print(\"Let's visualize the distance distribution of the LIGO event in the 90% credible region\")\n",
    "\n",
    "fig, ax = plt.subplots(ncols = 3, figsize = (15,8))\n",
    "\n",
    "\n",
    "ax[0].hist(data_90[\"DIST_Z\"], bins = \"auto\", color = \"orange\")\n",
    "ax[1].hist(data_90[\"DIST_Z\"], bins = \"auto\", color = \"green\")\n",
    "ax[2].hist(skymap_90[\"DISTMU\"], bins = \"auto\", color = \"blue\")\n",
    "\n",
    "\n",
    "for axis in ax:\n",
    "    axis.set_xlabel(\"distance bin (Mpc)\")\n",
    "\n",
    "ax[0].set_ylabel(\"object count\")\n",
    "ax[1].set_ylabel(\"object count\")\n",
    "ax[2].set_ylabel(\"pixel count\")\n",
    "\n",
    "ax[1].set_xlim(0,1000)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(skymap_90), len(data_90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To reduce our data even more, lets cut out the unreasonably far away objects\n",
    "\n",
    "We simply take maximum and minimum distance we get from the LIGO skymap (i.e. DISTMU+-DISTERR) and check, whether every galaxy in that pixel falls inside this range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_selection_mask = data_90[\"DIST_Z\"] - data_90[\"DIST_Z_ERR\"] <  data_90[\"DISTMU\"] + data_90[\"DISTSIGMA\"]\n",
    "z_selection_mask &= data_90[\"DIST_Z\"] + data_90[\"DIST_Z_ERR\"] >  data_90[\"DISTMU\"] - data_90[\"DISTSIGMA\"]\n",
    "\n",
    "data_90_z = data_90[z_selection_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_obj = np.size(np.where(z_selection_mask == True))\n",
    "deleted_obj = np.size(np.where(z_selection_mask == False))\n",
    "\n",
    "print(f\"The z-range selection has deleted {deleted_obj} objects from originally {deleted_obj+remaining_obj} in the data set\")\n",
    "print(f\"Therefore, we have {remaining_obj} objects left in the search region\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_prob_dens = np.max(data_90_z[\"PROBDENSITY\"])\n",
    "print(f\"Our highest remaing probability density is {highest_prob_dens} compared to the originally highest prob density of {max_probdens_in_ligo}\")\n",
    "print(f\"The difference in prob density is {max_probdens_in_ligo-highest_prob_dens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_90_z[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get the apparent, absolute magnitudes and luminosities\n",
    "\n",
    "in order to do this: flux > 0 selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the apparent mags\n",
    "data_90_z = data_90_z[data_90_z[\"FLUX_Z\"] > 0]\n",
    "app_mag_z = gs.app_mag(data_90_z[\"FLUX_Z\"])\n",
    "flux_selection_cut = len(data_90_z)\n",
    "print(f\"We have {flux_selection_cut} objects remaining in our catalog, i.e. {remaining_obj-flux_selection_cut} had to be cut out due to bad fluxes\")\n",
    "\n",
    "# get the absolute mags\n",
    "abs_mag_z = gs.abs_mag(app_mag_z, data_90_z[\"Z\"], 0,0,0)\n",
    "\n",
    "# get the luminosities\n",
    "lums_z = gs.lum_z(abs_mag_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_f = hstack([data_90_z, Table({\"APP_MAG_Z\": app_mag_z}), Table({\"ABS_MAG_Z\": abs_mag_z}), Table({\"LUM_Z\": lums_z})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_f[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lum_z = np.max(data_f[\"LUM_Z\"])*u.W\n",
    "prob_dens_max_lum = data_f[data_f[\"LUM_Z\"] == max_lum_z][\"PROBDENSITY\"]\n",
    "print(f\"The maximum luminosity in our cataloge in the z-band is {max_lum_z} with a prob density of {prob_dens_max_lum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg_errfc = abs(data_f[\"DIST_Z\"]-data_f[\"DISTMU\"])/(data_f[\"DIST_Z_ERR\"]**2+data_f[\"DISTSIGMA\"]**2)\n",
    "P_gal_unnorm = data_f[\"LUM_Z\"]*data_f[\"PROBDENSITY\"]*(1-sc.special.erf(arg_errfc))\n",
    "#dist_mean*u.Mpc, dist_std*u.Mpc\n",
    "arg_errfc_c = abs(data_f[\"DIST_Z\"].value-dist_mean)/(data_f[\"DIST_Z_ERR\"].value**2+dist_std**2)\n",
    "P_gal_unnorm_const = data_f[\"LUM_Z\"]*data_f[\"PROBDENSITY\"]*(1-sc.special.erf(arg_errfc_c))\n",
    "\n",
    "# now lets quickly normalize\n",
    "\n",
    "P_gal_sum = np.sum(P_gal_unnorm)\n",
    "P_gal_const_sum = np.sum(P_gal_unnorm_const)\n",
    "\n",
    "P_gal = 1/P_gal_sum * P_gal_unnorm\n",
    "P_gal_const = 1/P_gal_const_sum * P_gal_unnorm_const"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_f = hstack([data_f, Table({\"P_GAL\": P_gal.value}), Table({\"P_GAL_C\": P_gal_const.value})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_f[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Analyse the data: How much luminosity do we cover? Which galaxies should we observe?\n",
    "\n",
    "Lets look into what information we can retrieve from here. First we define, how many galaxies we could observe (N_galaxies).\n",
    "\n",
    "We then calculate the luminosity (lum) from all the galaxies in this area, i.e. the total luminosity. From there do some comparison between the covered luminosity and total luminosity depending on the ranking by probability:\n",
    "- simple \"Maximum lum that could be covered\" by looking at the brightest galaxies\n",
    "- Calculate the probability this way (from https://arxiv.org/pdf/1710.05452.pdf w/o normalization), both using a static and variable distance and error: $$P_{gal} = k^{-1}\\tilde{L_z}\\cdot P_{2D}\\left(1-\\text{erf}\\left(\\frac{|D_{Gal}-D_{LVC}|}{\\sigma_{D,gal}^{2}+\\sigma_{D,LVC}^{2}}\\right)\\right)$$\n",
    "- others may follow...\n",
    "\n",
    "Since we have the actual luminosity (here in z-band though), we don't need to again calculate the $$\\tilde{L}_{gal}$$ values. In the mentioned paper, they only use a fixed distance and error for the LIGO data. I will try both, to get an idea what the differences are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_galaxies = 20 # how many galaxies we can cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_lum = np.sum(data_f[\"LUM_Z\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare lum with luminosity ranking\n",
    "\n",
    "https://iopscience.iop.org/article/10.3847/0067-0049/226/1/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sort_by_lums = data_f.group_by(\"LUM_Z\")\n",
    "data_sort_by_lums = data_sort_by_lums[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lum_after_N_highest = np.sum(data_sort_by_lums[\"LUM_Z\"][:N_galaxies])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The theoretical maximum luminosity we could cover is {lum_after_N_highest}, i.e. {lum_after_N_highest*100/total_lum} % of the total luminosity\")\n",
    "print(\"The TARGETID values of these galaxies are: \\n\", data_sort_by_lums[\"TARGETID\"][:N_galaxies])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare lum with P_Gal ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sort_by_PGal = data_f.group_by(\"P_GAL\")\n",
    "data_sort_by_PGal = data_sort_by_PGal[::-1]\n",
    "\n",
    "lum_after_N_PGal = np.sum(data_sort_by_PGal[\"LUM_Z\"][:N_galaxies])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The maximum luminosity we cover when ranked by P_gal is {lum_after_N_PGal}, i.e. {lum_after_N_PGal*100/total_lum}% of the total luminosity\") \n",
    "print(\"The TARGETID values of these galaxies are: \\n\", data_sort_by_PGal[\"TARGET_RA\", \"TARGET_DEC\", \"SERSIC\", \"TARGETID\", \"P_GAL\"][:N_galaxies])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sort_by_PGal_C = data_f.group_by(\"P_GAL_C\")\n",
    "data_sort_by_PGal_C = data_sort_by_PGal_C[::-1]\n",
    "\n",
    "lum_after_N_PGal_C = np.sum(data_sort_by_PGal_C[\"LUM_Z\"][:N_galaxies])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The maximum luminosity we cover when ranked by P_gal_C is {lum_after_N_PGal_C}, i.e. {lum_after_N_PGal_C*100/total_lum}% of the total luminosity\") \n",
    "print(\"The TARGETID values of these galaxies are: \\n\", data_sort_by_PGal_C[\"TARGETID\", \"P_GAL\"][:N_galaxies])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sort_by_PGal[\"TARGET_RA\", \"TARGET_DEC\", \"SERSIC\", \"TARGETID\", \"P_GAL\"][:N_galaxies].write('PGAL_S200129.ecsv', delimiter=',', format='ascii', overwrite = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MambaTest)",
   "language": "python",
   "name": "mambatest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
