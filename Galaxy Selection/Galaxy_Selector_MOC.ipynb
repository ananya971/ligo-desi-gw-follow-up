{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Galaxy Selector MOC\n",
    "\n",
    "**Designed to work with Multi-Ordering-Sky-Maps and both burst (i.e. no distance) and normal LVK maps**\n",
    "\n",
    "\n",
    "\n",
    "Select the galaxies to observe by WWFI in this simple manner:\n",
    "- get the LIGO event data\n",
    "- read the LIGO event data into the notebook, extract some information\n",
    "- read the DESI database into the notebook, \"clean\" the data\n",
    "- get the 99% credible regions for the event, add them as a column to the data\n",
    "- only keep data within the 99% region\n",
    "- calculate all the luminosities (& more)\n",
    "- rank them by luminosity (for now just print the TARGETID), 3D/2D localization and with a luminosity-distance 2D dependant counterpart likelihood\n",
    "\n",
    "(some) Sources: \n",
    "- https://iopscience.iop.org/article/10.3847/0067-0049/226/1/10\n",
    "- https://emfollow.docs.ligo.org/userguide/tutorial/skymaps.html\n",
    "- https://emfollow.docs.ligo.org/userguide/tutorial/multiorder_skymaps.html\n",
    "- https://arxiv.org/pdf/1710.05452.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import all libraries, load GW data and DESI data and extract basic info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### imports, GW data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import scipy as sc\n",
    "from scipy.integrate import quad\n",
    "\n",
    "\n",
    "from astropy.io import fits, ascii\n",
    "from astropy.table import Table, hstack, QTable\n",
    "from astropy import table\n",
    "from astropy.coordinates import SkyCoord, EarthLocation, AltAz\n",
    "\n",
    "import astropy_healpix as ah\n",
    "\n",
    "\n",
    "import psycopg2\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "from astropy.cosmology import FlatLambdaCDM\n",
    "from astropy.cosmology import Planck18, z_at_value\n",
    "from astropy.coordinates import Distance, SkyCoord\n",
    "from astropy import units as u\n",
    "\n",
    "import astropy.constants as asc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import requests as rq\n",
    "\n",
    "import gasel as gs\n",
    "\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from datetime import date\n",
    "from suntimes import SunTimes\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(to get DESI data) you should run scp dgruen@perlmutter.nersc.gov:/pscratch/sd/j/jgassert/good_z_data.fits /project/ls-gruen/ligodesi/\n"
     ]
    }
   ],
   "source": [
    "# Get DESI data\n",
    "print(\"(to get DESI data) you should run scp dgruen@perlmutter.nersc.gov:/pscratch/sd/j/jgassert/good_z_data.fits /project/ls-gruen/ligodesi/\")\n",
    "\n",
    "# specify the superevent name:\n",
    "superevent_name = \"S200225q\"\n",
    "\n",
    "# pipeline - bayestar is preferred if available, otherwise maybe use olib? bilby? cwb? whatever these mean\n",
    "pipeline_type = \"\"\n",
    "\n",
    "if pipeline_type == \"cwb\":\n",
    "    # bursts probably only for very nearby core-collapse or Andromeda events, maybe cut at 1Mpc or something of that order? -> check if there is a very nearby galaxy\n",
    "        # think about which pipeline detected it!\n",
    "    # or very massive bbh but then hope is that cbc would detect as well\n",
    "    \n",
    "    modeled_search = False\n",
    "else:\n",
    "    modeled_search = True\n",
    "\n",
    "# storage directory\n",
    "# storage = \"/project/ls-gruen/ligodesi/\"\n",
    "# storage_desi_data = storage\n",
    "storage = \"/global/homes/j/jgassert/ligo-desi-gw-follow-up/Galaxy Selection/Files/\" # Julius on NERSC\n",
    "storage_desi_data = \"/pscratch/sd/j/jgassert/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no local skymap...\n",
      "...will download LIGO event file from superevent S200225q, corresponding file URL is https://gracedb.ligo.org/api/superevents/S200225q/files/.LALInference.fits.gz and save it in .../Files/\n",
      "HTTP status 404\n"
     ]
    }
   ],
   "source": [
    "path = storage+f\"{superevent_name}{pipeline_type}.LALInference.fits.gz\" # 5min after alert this should be there\n",
    "\n",
    "try:\n",
    "    skymap = QTable.read(path)\n",
    "    print(\"read skymap from local file\")\n",
    "    \n",
    "except:\n",
    "    download_url = f\"https://gracedb.ligo.org/api/superevents/{superevent_name}/files/{pipeline_type}.LALInference.fits.gz\"\n",
    "    save_path = storage+f\"{superevent_name}{pipeline_type}.LALInference.fits.gz\"\n",
    "    print(\"no local skymap...\")\n",
    "    print(f\"...will download LIGO event file from superevent {superevent_name}, corresponding file URL is {download_url} and save it in .../Files/\")\n",
    "    \n",
    "    response = rq.get(download_url, allow_redirects = True)\n",
    "    print(\"HTTP status\",response.status_code)\n",
    "    if response.status_code == 200:\n",
    "        with open(save_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "            \n",
    "        skymap = QTable.read(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifiy the path where the data is located and then load the data (this loads everything, just have a look)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><i>QTable length=5</i>\n",
       "<table id=\"table140089386100784\" class=\"table-striped table-bordered table-condensed\">\n",
       "<thead><tr><th>UNIQ</th><th>PROBDENSITY</th><th>DISTMU</th><th>DISTSIGMA</th><th>DISTNORM</th></tr></thead>\n",
       "<thead><tr><th></th><th>1 / sr</th><th>Mpc</th><th>Mpc</th><th>1 / Mpc2</th></tr></thead>\n",
       "<thead><tr><th>int64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th></tr></thead>\n",
       "<tr><td>1024</td><td>2.0628782133841405e-12</td><td>-1012.2049520497791</td><td>376.82374575948757</td><td>0.011095199813917983</td></tr>\n",
       "<tr><td>1025</td><td>8.27362108428298e-12</td><td>166.23094482047838</td><td>153.23732378694845</td><td>2.014040931456908e-05</td></tr>\n",
       "<tr><td>1026</td><td>1.4736272931268072e-12</td><td>inf</td><td>1.0</td><td>0.0</td></tr>\n",
       "<tr><td>1027</td><td>6.780677710008301e-12</td><td>102.07357208021745</td><td>116.54675869191472</td><td>4.412607201084553e-05</td></tr>\n",
       "<tr><td>1028</td><td>5.79319209390381e-11</td><td>78.61969670578218</td><td>210.35830952485827</td><td>2.582873160902654e-05</td></tr>\n",
       "</table></div>"
      ],
      "text/plain": [
       "<QTable length=5>\n",
       " UNIQ      PROBDENSITY       ...     DISTSIGMA             DISTNORM      \n",
       "              1 / sr         ...        Mpc                1 / Mpc2      \n",
       "int64        float64         ...      float64              float64       \n",
       "----- ---------------------- ... ------------------ ---------------------\n",
       " 1024 2.0628782133841405e-12 ... 376.82374575948757  0.011095199813917983\n",
       " 1025   8.27362108428298e-12 ... 153.23732378694845 2.014040931456908e-05\n",
       " 1026 1.4736272931268072e-12 ...                1.0                   0.0\n",
       " 1027  6.780677710008301e-12 ... 116.54675869191472 4.412607201084553e-05\n",
       " 1028   5.79319209390381e-11 ... 210.35830952485827 2.582873160902654e-05"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skymap[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get basic values from GW event\n",
    "\n",
    "We now extract some basic information  from the healpix data:\n",
    "- ipix_max: pixel with highest likelihood\n",
    "- npix: total number of pixels\n",
    "- ra, dec: ra and dec of the pixel with the highest likelihood\n",
    "\n",
    "Then we open the full fits file and extract some more basic info from the header:\n",
    "- dist_mean: mean distance of the GW event\n",
    "- dist_std: error of the distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIQ ID of highest prob sky localization:  1494052\n"
     ]
    }
   ],
   "source": [
    "# most probable sky location\n",
    "i = np.argmax(skymap[\"PROBDENSITY\"])\n",
    "uniq = skymap[i][\"UNIQ\"]\n",
    "print(\"UNIQ ID of highest prob sky localization: \", uniq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest probability at (RA, dec):  108.67346938775509 48.82520674420733 with prob [1/deg^2] of:  0.005534703004111113 and UNIQ ID:  1494052\n"
     ]
    }
   ],
   "source": [
    "# calculate the most probable pixel, convert to RA and dec\n",
    "level_max, ipix_max = ah.uniq_to_level_ipix(uniq)\n",
    "nside = ah.level_to_nside(level_max)\n",
    "ra, dec = ah.healpix_to_lonlat(ipix_max, nside, order = \"nested\")\n",
    "print(\"Highest probability at (RA, dec): \", ra.deg, dec.deg, \"with prob [1/deg^2] of: \", skymap[i]['PROBDENSITY'].to_value(u.deg**-2), \"and UNIQ ID: \", skymap[i][\"UNIQ\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now open the fits file and read basics info from the header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/global/homes/j/jgassert/ligo-desi-gw-follow-up/Galaxy Selection/Files/S200225q.LALInference.fits.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m modeled_search:\n\u001b[0;32m----> 2\u001b[0m     fits_gw \u001b[38;5;241m=\u001b[39m \u001b[43mfits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     header \u001b[38;5;241m=\u001b[39m fits_gw[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mheader\n\u001b[1;32m      4\u001b[0m     dist_mean \u001b[38;5;241m=\u001b[39m header[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDISTMEAN\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/MambaTest/lib/python3.10/site-packages/astropy/io/fits/hdu/hdulist.py:214\u001b[0m, in \u001b[0;36mfitsopen\u001b[0;34m(name, mode, memmap, save_backup, cache, lazy_load_hdus, ignore_missing_simple, use_fsspec, fsspec_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m name:\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmpty filename: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mHDUList\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromfile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_backup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlazy_load_hdus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_missing_simple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_fsspec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_fsspec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfsspec_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfsspec_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/MambaTest/lib/python3.10/site-packages/astropy/io/fits/hdu/hdulist.py:482\u001b[0m, in \u001b[0;36mHDUList.fromfile\u001b[0;34m(cls, fileobj, mode, memmap, save_backup, cache, lazy_load_hdus, ignore_missing_simple, **kwargs)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfromfile\u001b[39m(\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    472\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    473\u001b[0m ):\n\u001b[1;32m    474\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;124;03m    Creates an `HDUList` instance from a file-like object.\u001b[39;00m\n\u001b[1;32m    476\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;124;03m    documentation for details of the parameters accepted by this method).\u001b[39;00m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_readfrom\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfileobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfileobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_backup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_backup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_missing_simple\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_missing_simple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlazy_load_hdus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlazy_load_hdus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/MambaTest/lib/python3.10/site-packages/astropy/io/fits/hdu/hdulist.py:1170\u001b[0m, in \u001b[0;36mHDUList._readfrom\u001b[0;34m(cls, fileobj, data, mode, memmap, cache, lazy_load_hdus, ignore_missing_simple, use_fsspec, fsspec_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fileobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1168\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fileobj, _File):\n\u001b[1;32m   1169\u001b[0m         \u001b[38;5;66;03m# instantiate a FITS file object (ffo)\u001b[39;00m\n\u001b[0;32m-> 1170\u001b[0m         fileobj \u001b[38;5;241m=\u001b[39m \u001b[43m_File\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1171\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfileobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1173\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmemmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1174\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_fsspec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_fsspec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfsspec_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfsspec_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1178\u001b[0m     \u001b[38;5;66;03m# The Astropy mode is determined by the _File initializer if the\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m     \u001b[38;5;66;03m# supplied mode was None\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m     mode \u001b[38;5;241m=\u001b[39m fileobj\u001b[38;5;241m.\u001b[39mmode\n",
      "File \u001b[0;32m~/.conda/envs/MambaTest/lib/python3.10/site-packages/astropy/io/fits/file.py:218\u001b[0m, in \u001b[0;36m_File.__init__\u001b[0;34m(self, fileobj, mode, memmap, overwrite, cache, use_fsspec, fsspec_kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open_fileobj(fileobj, mode, overwrite)\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fileobj, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m)):\n\u001b[0;32m--> 218\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open_filename\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfileobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open_filelike(fileobj, mode, overwrite)\n",
      "File \u001b[0;32m~/.conda/envs/MambaTest/lib/python3.10/site-packages/astropy/io/fits/file.py:636\u001b[0m, in \u001b[0;36m_File._open_filename\u001b[0;34m(self, filename, mode, overwrite)\u001b[0m\n\u001b[1;32m    633\u001b[0m ext \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_read_compressed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, magic, mode, ext\u001b[38;5;241m=\u001b[39mext):\n\u001b[0;32m--> 636\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIO_FITS_MODES\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose_on_error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;66;03m# Make certain we're back at the beginning of the file\u001b[39;00m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;66;03m# BZ2File does not support seek when the file is open for writing, but\u001b[39;00m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;66;03m# when opening a file for write, bz2.BZ2File always truncates anyway.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/global/homes/j/jgassert/ligo-desi-gw-follow-up/Galaxy Selection/Files/S200225q.LALInference.fits.gz'"
     ]
    }
   ],
   "source": [
    "if modeled_search:\n",
    "    fits_gw = fits.open(path)\n",
    "    header = fits_gw[1].header\n",
    "    dist_mean = header[\"DISTMEAN\"]\n",
    "    dist_std = header[\"DISTSTD\"]\n",
    "    print(\"The dist_mean and dist_std values: \", dist_mean*u.Mpc, dist_std*u.Mpc)\n",
    "    most_likely_z = z_at_value(Planck18.luminosity_distance, dist_mean*u.Mpc, zmax = 5)\n",
    "    most_likely_z_std = z_at_value(Planck18.luminosity_distance, dist_std*u.Mpc, zmax = 5)\n",
    "    print(f\"This corresponds to the most likely z {most_likely_z} and its error {most_likely_z_std}\")\n",
    "else:\n",
    "    print(\"As this is an unmodeled search there is no distance available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Get DESI data\n",
    "\n",
    "Load the full daily data from the file, which only includes good redshift objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "follow_up_others = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if follow_up_others:\n",
    "    center_ra = 235.9839\n",
    "    center_dec = 15.2248\n",
    "    radius = 1/360\n",
    "    z_others = 0\n",
    "    z_others_std = 0.4\n",
    "    \n",
    "    sigma_accuracy = 3\n",
    "    \n",
    "    data = gs.db_doall(center_ra, center_dec, radius)\n",
    "    \n",
    "    #ii = data[\"Z\"]-sigma_accuracy*data[\"ZERR\"]<z_others+sigma_accuracy*z_others_std\n",
    "    #ii &= data[\"Z\"]+sigma_accuracy*data[\"ZERR\"]>z_others-sigma_accuracy*z_others_std\n",
    "    \n",
    "    #data = data[ii]\n",
    "    \n",
    "    \n",
    "else:\n",
    "    data = Table.read(storage_desi_data+\"good_z_data.fits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Do all the calculations, selections,...\n",
    "\n",
    "- turn the daily DESI data into a Table\n",
    "- do some basic data selection stuff: only keep good + positive redshifts, only positive values of r-band flux and eliminate duplicates (right now simply takes the first entry)\n",
    "- add the ipix (healpix) pixel value for each target\n",
    "- calculate the probability for each target\n",
    "- add these values to the Table (\"PROB\"), select only targets within the 99% credible region (i.e. with a certainty of 99% the GW event is inside this region)\n",
    "- calculate the distances and its errors from the redshift, add to the data table\n",
    "- calculate the 3D probability just like here: https://iopscience.iop.org/article/10.3847/0067-0049/226/1/10 (§4) and add these values to the table\n",
    "- sort the table by its 3D probability (descending)\n",
    "- calculate absolute and apparent magnitude, luminosity and add to table\n",
    "\n",
    "The final step is to include the calculation of the most likely host galaxy from https://arxiv.org/pdf/1710.05452.pdf (GW170817); these values are then also added to the table as \"P_GAL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### data clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = data[data[\"Z\"]> 0] # this migh have 2 reasons\n",
    "#data = data[data[\"ZWARN\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = data[data['FLUX_Z']>0]\n",
    "# data = table.unique(data, keys = \"TARGETID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data[\"TARGETID\"]>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"We have {len(data)} objects in the full catalog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should also get rid of stars, since they wont be the host of a GW event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data[\"SPECTYPE\"] != \"STAR\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Calculate the healpix id for every object in the catalog and its probability; append this data to the Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_level = 29 # highest possible HEALPix resolution that can be represented in a 64-bit signed integer\n",
    "max_nside = ah.level_to_nside(max_level)\n",
    "level, ipix = ah.uniq_to_level_ipix(skymap[\"UNIQ\"])\n",
    "\n",
    "index = ipix*(2**(max_level-level))**2\n",
    "\n",
    "sorter = np.argsort(index)\n",
    "\n",
    "# this is the NESTED pixel index of the target sky location, NOT THE UNIQ ID\n",
    "match_ipix = ah.lonlat_to_healpix(data[\"TARGET_RA\"]*u.deg, data[\"TARGET_DEC\"]*u.deg, max_nside, order='nested')\n",
    "\n",
    "# here we get the pixel index (i.e. where the entry is in the table, also NOT THE UNIQ ID) of each target\n",
    "sorter_i = sorter[np.searchsorted(index, match_ipix, side='right', sorter=sorter) - 1]\n",
    "probdensity = skymap[sorter_i]['PROBDENSITY'].to_value(u.deg**-2)\n",
    "\n",
    "# now we calculate the UNIQ ID for all our targets:\n",
    "\n",
    "uniq = skymap[\"UNIQ\"][sorter_i]\n",
    "#to make things easier in future, I will now also append the UNIQ pixels DISTMU, DISTSIGMA and DISTNORM values. However, be careful, since these DO NOT CORRESPOND THE TARGETS IN DESI DIRECTLY!\n",
    "\n",
    "if modeled_search:\n",
    "    distmu = skymap[sorter_i][\"DISTMU\"]\n",
    "    distsigma = skymap[sorter_i][\"DISTSIGMA\"]\n",
    "    distnorm = skymap[sorter_i][\"DISTNORM\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(UNIQ), len(data), len(skymap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = data.keep_columns([\"TARGETID\", \"TARGET_RA\", \"TARGET_DEC\", \"TILEID\", \"NIGHT\", \"Z\", \"ZERR\", \"ZWARN\", \"DELTACHI2\", \"FLUX_Z\", \"BGS_TARGET\", \"EBV\", \"SERSIC\", \"MWS_TARGET\", \"FILENAME\"])\n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if modeled_search:\n",
    "    data = hstack([data, Table({\"UNIQ\": uniq}), Table({\"PROBDENSITY\": probdensity}), Table({\"DISTMU\": distmu}), Table({\"DISTSIGMA\": distsigma}), Table({\"DISTNORM\": distnorm})])\n",
    "else: \n",
    "    data = hstack([data, Table({\"UNIQ\": uniq}), Table({\"PROBDENSITY\": probdensity})])\n",
    "    print(f\"The table will not contain any distance info since there is none available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The highest and lowest probability values in our catalog are: {np.max(probdensity)} and {np.min(probdensity)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Sort the skymap table and get 99% credible region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the table\n",
    "skymap_sort = Table(skymap, copy = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether copy worked:\n",
    "# skymap_sort, skymap\n",
    "# np.max(skymap[\"UNIQ\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now sort the skymap by its probability (descending)\n",
    "skymap_sort.sort('PROBDENSITY', reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this shows the first entries sorted be their probability\n",
    "skymap_sort[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now get 99% credible region area\n",
    "\n",
    "level, ipix = ah.uniq_to_level_ipix(skymap_sort['UNIQ'])\n",
    "pixel_area = ah.nside_to_pixel_area(ah.level_to_nside(level)) # this is the area each pixel contains\n",
    "\n",
    "# calculate the probability per pixel (careful: since the pixels differ in size, this is not neceseraliy a useful quantity\n",
    "prob = pixel_area * skymap_sort['PROBDENSITY']\n",
    "\n",
    "# calc cumsum of probabilities and get index of pixel that sums up to 0.99\n",
    "cumprob = np.cumsum(prob)\n",
    "i = cumprob.searchsorted(0.99)\n",
    "\n",
    "# print area\n",
    "area_99 = pixel_area[:i].sum()\n",
    "area_99_deg = area_99.to_value(u.deg**2)\n",
    "print(f\"The 99% search area is {area_99_deg}°large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### can be deleted, just tried things out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skymap_sort = hstack([skymap_sort, Table({\"PROB\": prob})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skymap_sort[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skymap_prob = Table(skymap_sort, copy = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skymap_prob.sort(\"PROB\", reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skymap_prob[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compare Tables, only leave 99% credible region in DESI data table\n",
    "\n",
    "Now we can get a Table that only contains the 99% credible region. From there we only leave the matching objects in the DESI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we simply select only the entries that are within the 99% credible region\n",
    "skymap_99 = skymap_sort[:i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This is the number of UNIQ ID of objects which should lie within the 99% credible region (NOT THE NUMBER OF OBJECTS, as there can be multiple objects per UNIQ ID):\", len(np.intersect1d(skymap_99[\"UNIQ\"], data[\"UNIQ\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.in1d(data[\"UNIQ\"], skymap_99[\"UNIQ\"])\n",
    "data_99 = data[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"We now have {len(data_99)} objects in the 99% credible region\")\n",
    "print(\"Check if the region from where the galaxies have been selected makes any sense (RA; DEC): \", np.min(data_99[\"TARGET_RA\"]), np.max(data_99[\"TARGET_RA\"]), np.min(data_99[\"TARGET_DEC\"]), np.max(data_99[\"TARGET_DEC\"]))\n",
    "\n",
    "max_probdens_in_desi = np.max(data_99[\"PROBDENSITY\"])\n",
    "max_probdens_in_ligo = np.max(skymap[\"PROBDENSITY\"]).to_value(u.deg**-2)\n",
    "print(f\"We can also check whether we have at least one galaxy in the UNIQ pixel with the highest probability from LIGO...\")\n",
    "print(f\"This is {np.allclose(max_probdens_in_desi, max_probdens_in_ligo)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_99.sort('PROBDENSITY', reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_99[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if modeled_search:\n",
    "    print(\"We have now performed all the necessary steps to get the localization selection in 2D, i.e. the angle on the sky\")\n",
    "    min_z = np.min(data_99[\"Z\"])\n",
    "    max_z = np.max(data_99[\"Z\"])\n",
    "    print(f\"However, we have not used the distance data at all. Therefore our redshifts range from {min_z} to {max_z}\")\n",
    "else:\n",
    "    print(f\"Usually we would be performing steps to cut out objects that don't meet the z-range/distance from LIGO, since there is no distance, we - at least for now include everything!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Calculate all the distances of the objects from the redshifts\n",
    "\n",
    "We want to be conservative in estimating the distance from redshifts, therefore we will include different cosmologies.\n",
    "\n",
    "Our cosmologies include:\n",
    "- H0 = 68.20 ± 0.81 km s−1Mpc−1, from eBOSS https://arxiv.org/pdf/2007.08991.pdf\n",
    "- DES Y3 https://arxiv.org/abs/2105.13549\n",
    "- SH0ES https://pantheonplussh0es.github.io/\n",
    "\n",
    "Now we calculate the arithmetic mean of our distances and its standard deviation:\n",
    "$$distances_{mean} = \\frac{dist(z_{cosmo_{max}})+dist(z_{cosmo_{min}})}{2}$$\n",
    "and accordingly the standard deviation (n=2):\n",
    "$$\\sigma = \\sqrt{\\frac{1}{n-1}\\sum\\left(dist_i-distances_{mean}\\right)^2}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distances = Distance(z=data_99[\"Z\"], cosmology=Planck18)\n",
    "#distances_err = Distance(z=data_99[\"ZERR\"], cosmology=Planck18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if modeled_search:\n",
    "    # first define the cosmological maximum and minimum parameters\n",
    "    omega_M_max = 0.339+0.032 #(from DES)\n",
    "    omega_M_min = 0.299-2*0.016 #from eBOSS\n",
    "\n",
    "    H_0_max = 73.04+2*1.04 #from SH0ES\n",
    "    H_0_min = 67.4-2*0.5 #from Planck\n",
    "    \n",
    "    # then calculate min and max luminosity distance from the redshifts\n",
    "    cosmo_min_dist = FlatLambdaCDM(H0=H_0_max, Om0=omega_M_max)\n",
    "    cosmo_max_dist = FlatLambdaCDM(H0=H_0_min, Om0=omega_M_min)\n",
    "\n",
    "    dists_max = Distance(z=data_99[\"Z\"], cosmology=cosmo_max_dist)\n",
    "    dists_min = Distance(z=data_99[\"Z\"], cosmology=cosmo_min_dist)\n",
    "    \n",
    "    # and of course also calculate the maximum and minimum error from the redshfit uncertainty\n",
    "    dists_err_min = Distance(z=data_99[\"ZERR\"], cosmology=cosmo_max_dist)\n",
    "    dists_err_max = Distance(z=data_99[\"ZERR\"], cosmology=cosmo_min_dist)\n",
    "    \n",
    "    # now we get the mean distance and its error\n",
    "    dists_mean = (dists_max+dists_min)/2\n",
    "    dists_err_mean = (dists_err_max+dists_err_min)/2  # this is our sigma for the redshift error\n",
    "    \n",
    "    # calculate the stdev of the distance depending on the cosmology\n",
    "    dists_mean_stddev = np.sqrt((dists_max-dists_mean)**2+(dists_min-dists_mean)**2)\n",
    "    \n",
    "    # now stack all the data\n",
    "    data_99 = hstack([data_99, Table({\"DIST_Z_MEAN\": dists_mean}), Table({\"DIST_Z_ERR\": dists_err_mean}), Table({\"DIST_Z_COSMO_SIGMA\": dists_mean_stddev})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dists_mean[:5], dists_mean_stddev[:5], dists_mean_stddev[:5]/dists_mean[:5], np.mean(dists_mean_stddev/dists_mean), np.max(dists_mean_stddev/dists_mean), np.min(dists_mean_stddev/dists_mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_99[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Lets check how much the distance measurement changes in the 99% credible region of LIGO and compare to our DESI data\n",
    "\n",
    "We should see a lot more data in DESI (since we haven't done any redshift selection and there can be multiple objects per GW-pixel). Be carfeul, we are comparing actual object counts with the number of pixels.\n",
    "\n",
    "\n",
    "We still see, that most of the GW localization is roughly between 780Mpc and 950Mpc. DESI definitely has a most of its data in this range as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if modeled_search:\n",
    "    plt_skymap = skymap_99[skymap_99[\"DISTMU\"]<np.inf]\n",
    "\n",
    "    dist_max_99 = np.max(plt_skymap[\"DISTMU\"])*u.Mpc\n",
    "    dist_min_99 = np.min(plt_skymap[\"DISTMU\"])*u.Mpc\n",
    "    print(f\"Our LIGO distances range from {dist_min_99} to {dist_max_99}\")\n",
    "    print(\"Let's visualize the distance distribution of the LIGO event in the 99% credible region\")\n",
    "\n",
    "    fig, ax = plt.subplots(ncols = 2, figsize = (15,8))\n",
    "\n",
    "\n",
    "    ax[0].hist(data_99[\"DIST_Z_MEAN\"], bins = \"auto\", color = \"orange\", label = \"DESI data\")\n",
    "    ax[1].hist(plt_skymap[\"DISTMU\"], bins = \"auto\", color = \"blue\", label = \"LIGO data\")\n",
    "\n",
    "\n",
    "    for axis in ax:\n",
    "        axis.set_xlabel(\"distance bin (Mpc)\")\n",
    "        axis.set_xlim(dist_min_99/u.Mpc, dist_max_99/u.Mpc)\n",
    "        axis.legend()\n",
    "\n",
    "    ax[0].set_ylabel(\"object count\")\n",
    "    ax[1].set_ylabel(\"pixel count\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(skymap_99), len(data_99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### To reduce our data even more, lets cut out the unreasonably far away objects\n",
    "\n",
    "We simply take maximum and minimum distance we get from the LIGO skymap (i.e. DISTMU+-DISTERR) and check, whether every galaxy in that pixel falls inside this range.\n",
    "\n",
    "*Note: We do this only for our own GW-follow ups*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if follow_up_others == False and modeled_search == True:\n",
    "    z_selection_mask = data_99[\"DIST_Z_MEAN\"] - 2*data_99[\"DIST_Z_ERR\"] - 2*data_99[\"DIST_Z_COSMO_SIGMA\"] <  data_99[\"DISTMU\"] + 2*data_99[\"DISTSIGMA\"]\n",
    "    z_selection_mask &= data_99[\"DIST_Z_MEAN\"] + 2*data_99[\"DIST_Z_ERR\"] + 2*data_99[\"DIST_Z_COSMO_SIGMA\"] >  data_99[\"DISTMU\"] - 2*data_99[\"DISTSIGMA\"]\n",
    "\n",
    "    data_99_z = data_99[z_selection_mask]\n",
    "\n",
    "    remaining_obj = np.size(np.where(z_selection_mask == True))\n",
    "    deleted_obj = np.size(np.where(z_selection_mask == False))\n",
    "\n",
    "    print(f\"The z-range selection has deleted {deleted_obj} objects from originally {deleted_obj+remaining_obj} in the data set\")\n",
    "    print(f\"Therefore, we have {remaining_obj} objects left in the search region\")\n",
    "\n",
    "    highest_prob_dens = np.max(data_99_z[\"PROBDENSITY\"])\n",
    "    print(f\"Our highest remaing probability density is {highest_prob_dens} compared to the originally highest prob density of {max_probdens_in_ligo}\")\n",
    "    print(f\"The difference in prob density is {max_probdens_in_ligo-highest_prob_dens}\")\n",
    "\n",
    "    \n",
    "else:\n",
    "    data_99_z = data_99\n",
    "    remaining_obj = len(data_99_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_99_z[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### get the apparent, absolute magnitudes and luminosities\n",
    "\n",
    "in order to do this: flux > 0 selection  and gal ext correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the apparent mags\n",
    "orig_len = len(data_99_z)\n",
    "data_99_z = data_99_z[data_99_z[\"FLUX_R\"] > 0] # flux selection (only ones above 0)\n",
    "print(f\"had to cut out {len(data_99_z)-orig_len} objects due to bad fluxes\")\n",
    "data_99_z[\"MW_TRANSMISSION\"] = gs.mw_transmission_from_data_table(data = data_99_z, band = \"r\")\n",
    "#data_99_z[\"FLUX_R\"] = data_99_z[\"FLUX_R\"]/data_99_z[\"MW_TRANSMISSION\"]\n",
    "\n",
    "app_mag_r = gs.app_mag(data_99_z[\"FLUX_R\"], mw_transmission = data_99_z[\"MW_TRANSMISSION\"])\n",
    "flux_selection_cut = len(data_99_z)\n",
    "\n",
    "# get the absolute mags\n",
    "abs_mag_r = gs.abs_mag(app_mag_r, data_99_z[\"Z\"], 0,0,0)\n",
    "\n",
    "# get the luminosities\n",
    "lums_r = gs.lum(abs_mag_r, band = \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_f = hstack([data_99_z, Table({\"APP_MAG_R\": app_mag_r}), Table({\"ABS_MAG_R\": abs_mag_r}), Table({\"LUM_R\": lums_r})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_f[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Check the absolute and apparent magnitudes\n",
    "\n",
    "This is a sanity check to quickly check whether there might be some obvious problem in the data/derivation of magnitudes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(data_f) < 1000000:\n",
    "    # dauert sonst einfach ewig\n",
    "    fig, ax = plt.subplots(ncols = 3, figsize=(10,5))\n",
    "\n",
    "    plot_names = [\"APP_MAG_R\", \"ABS_MAG_R\", \"LUM_R\"]\n",
    "    cnt = 0\n",
    "    for axis in ax:\n",
    "        axis.hist(data_f[plot_names[cnt]], bins = \"auto\")\n",
    "        axis.set_yscale(\"log\")\n",
    "        cnt += 1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_abs_mags = True\n",
    "\n",
    "if inspect_abs_mags:\n",
    "    data_to_inspect = data_f[data_f[\"ABS_MAG_R\"]> -12]\n",
    "    fig, ax = plt.subplots(ncols = 5, figsize = (15,8))\n",
    "    \n",
    "    ax[0].hist(data_to_inspect[\"ABS_MAG_R\"], bins = \"auto\")\n",
    "    ax[0].set_xlabel(\"abs mag in r-band\")\n",
    "    ax[1].hist(data_to_inspect[\"MW_TRANSMISSION\"], bins = \"auto\")\n",
    "    ax[1].set_xlabel(\"MW_TRANSMISSION\")\n",
    "    ax[2].hist(data_to_inspect[\"APP_MAG_R\"], bins = \"auto\")\n",
    "    ax[2].set_xlabel(\"App mag r-band\")\n",
    "    ax[3].hist(data_to_inspect[\"DIST_Z_MEAN\"], bins = \"auto\")\n",
    "    ax[3].set_xlabel(\"distance from redshift\")\n",
    "    ax[4].hist(data_to_inspect[\"Z\"], bins = \"auto\")\n",
    "    ax[4].set_xlabel(\"redshift\")\n",
    "    \n",
    "    mean_redshift_pec_objects = np.mean(data_to_inspect[\"Z\"])\n",
    "    mean_distance_pec_objects = np.mean(data_to_inspect[\"DIST_Z_MEAN\"])*u.Mpc\n",
    "    min_redshift_pec_objects = np.min(data_to_inspect[\"Z\"])\n",
    "    min_distance_pec_objects = np.min(data_to_inspect[\"DIST_Z_MEAN\"])*u.Mpc\n",
    "    \n",
    "    data_to_inspect_len = len(data_to_inspect)\n",
    "    star_data_len = len(data_to_inspect[data_to_inspect[\"SPECTYPE\"] == \"STAR\"])\n",
    "    \n",
    "    print(f\"These objects have a mean redshift of {mean_redshift_pec_objects}, which corresponds to a mean distance of {mean_distance_pec_objects}\")\n",
    "    print(f\"The minimum redshift is {min_redshift_pec_objects}, i.e. {min_distance_pec_objects}\")\n",
    "    print(f\"According to WolframAlpha the closest galaxy has a distance of roughly 25000ly = 7655pc (not Mpc, simply pc), so any object below 0.07Mpc is definitely not a real galaxy\")\n",
    "    print(f\"This reminded me to check if we have selected galaxies only...\")\n",
    "    print(f\"We have {star_data_len} stars in our catalogue of peculiar objects, compared to a total number of {data_to_inspect_len} objects...\")\n",
    "                        \n",
    "    for axis in ax:\n",
    "        axis.set_ylabel(\"bin count\")\n",
    "        #axis.legend()\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.max(data_f[\"MW_TRANSMISSION\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### start calculating the P_Gal (the probability that galaxy X is the host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lum_r = np.max(data_f[\"LUM_R\"])*u.W\n",
    "prob_dens_max_lum = data_f[data_f[\"LUM_R\"] == max_lum_r][\"PROBDENSITY\"]\n",
    "print(f\"The maximum luminosity in our catalog in the r-band is {max_lum_r} with a prob density of {prob_dens_max_lum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if modeled_search:\n",
    "\n",
    "    \"\"\"arg_errfc = abs(data_f[\"DIST_Z_MEAN\"]-data_f[\"DISTMU\"])/(data_f[\"DIST_Z_ERR\"]**2+data_f[\"DISTSIGMA\"]**2+data_f[\"DIST_Z_COSMO_SIGMA\"].value**2)\n",
    "    P_gal_unnorm = data_f[\"LUM_R\"]*data_f[\"PROBDENSITY\"]*(1-sc.special.erf(arg_errfc))\n",
    "    #dist_mean*u.Mpc, dist_std*u.Mpc\n",
    "    arg_errfc_c = abs(data_f[\"DIST_Z_MEAN\"].value-dist_mean)/(data_f[\"DIST_Z_ERR\"].value**2+dist_std**2+data_f[\"DIST_Z_COSMO_SIGMA\"].value**2)\n",
    "    P_gal_unnorm_const = data_f[\"LUM_R\"]*data_f[\"PROBDENSITY\"]*(1-sc.special.erf(arg_errfc_c))\n",
    "\n",
    "    # now lets quickly normalize\n",
    "\n",
    "    P_gal_sum = np.sum(P_gal_unnorm)\n",
    "    P_gal_const_sum = np.sum(P_gal_unnorm_const)\n",
    "\n",
    "    P_gal = 1/P_gal_sum * P_gal_unnorm\n",
    "    P_gal_const = 1/P_gal_const_sum * P_gal_unnorm_const\"\"\"\n",
    "    \n",
    "    dist_delta = abs(data_f[\"DIST_Z_MEAN\"]-data_f[\"DISTMU\"])\n",
    "    combined_sigma2 =  data_f[\"DISTSIGMA\"]**2 + data_f[\"DIST_Z_COSMO_SIGMA\"].value**2 + data_f[\"DIST_Z_ERR\"]**2\n",
    "    P_gal_unnorm = data_f[\"LUM_R\"]*data_f[\"PROBDENSITY\"]/(np.sqrt(2*np.pi*combined_sigma2))*np.exp(-dist_delta**2/(combined_sigma2))\n",
    "    \n",
    "    P_gal_sum = np.sum(P_gal_unnorm)\n",
    "    P_gal = 1/P_gal_sum * P_gal_unnorm\n",
    "\n",
    "\n",
    "else:\n",
    "    P_gal_unnorm = data_f[\"LUM_R\"]*data_f[\"PROBDENSITY\"]\n",
    "    P_gal_sum = np.sum(P_gal_unnorm)\n",
    "    P_gal = 1/P_gal_sum * P_gal_unnorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get the missed fraction of DESI due to mass/luminosity incompleteness\n",
    "\n",
    "$$f_{missed} =  \\frac{\\int_{-\\infty}^{\\infty} dm 10^{-0.4m}\\left(1-\\frac{Hist}{Schechter}\\right)\\cdot Schechter}{\\int_{-\\infty}^{\\infty}dm 10^{-0.4m}\\cdot Schechter}$$\n",
    "\n",
    "We want to account for the fact that some region were covered in more detail by DESI than others..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if modeled_search:\n",
    "    \"\"\"\n",
    "    # get the Schechter model, see 1/V estimator, bin size of half a mag\n",
    "    from astropy.modeling.models import Schechter1D\n",
    "\n",
    "    phi_star = 0.0016745140734036254\n",
    "    m_star = -21.757361746831197\n",
    "    alpha = -1.1297780395774693\n",
    "\n",
    "    schechter_model = Schechter1D(phi_star, m_star, alpha)\n",
    "    \n",
    "    data_min_dist = np.min(data_f[\"DIST_Z_MEAN\"])*u.Mpc\n",
    "    data_max_dist = np.max(data_f[\"DIST_Z_MEAN\"])*u.Mpc\n",
    "    \n",
    "    abs_mag_max = np.max(data_f[\"ABS_MAG_R\"])\n",
    "    abs_mag_min = np.min(data_f[\"ABS_MAG_R\"])\n",
    "\n",
    "    bins = np.arange(abs_mag_min, abs_mag_max, 0.5)\n",
    "\n",
    "    hist, bins = np.histogram(data_f[\"ABS_MAG_R\"], bins =  bins)\n",
    "    \n",
    "    # normalize our histogram to our volume\n",
    "    volume = V_\n",
    "    hist = hist/(volume)\n",
    "    \n",
    "    #hist = hist/(area_99.to_value(u.deg**2)*(data_max_dist-data_min_dist).value*10**0) # completely wrong normalization\n",
    "    \n",
    "    print(f\"{abs_mag_min}, {abs_mag_max}, {data_min_dist}, {data_max_dist}, {(data_max_dist-data_min_dist).value*10**6}\")\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize = (5,2.5))\n",
    "    bins_center = bins[1:]-(bins[1]-bins[2])/2\n",
    "    ax.plot(bins_center, hist, label = \"wrongly normalized distribution\")\n",
    "    ax.plot(np.linspace(-25,-18), schechter_model(np.linspace(-25,-18)),  label = \"some schechter fit\", color = \"red\")\n",
    "\n",
    "    ax.set_yscale(\"log\")\n",
    "    plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if modeled_search:\n",
    "    def hist_func(M):\n",
    "        # https://stackoverflow.com/questions/2566412/find-nearest-value-in-numpy-array\n",
    "        # make sure array is a numpy array\n",
    "        # array = np.array(array)\n",
    "        idx = (np.abs(hist - M)).argmin()\n",
    "        val = hist[idx]\n",
    "        #print(f\"val {val}\")\n",
    "        return val\n",
    "\n",
    "    def num_func(M):\n",
    "        #print(f\"M is {M}\")\n",
    "        schechter_val = schechter_model(M)\n",
    "        #print(f\"schechter Val: {schechter_val}\")\n",
    "        lum_factor = 10**(-0.4*M)\n",
    "        #print(f\"lum factor {lum_factor}\")\n",
    "        weighting_coverage = (1-hist_func(M)/(schechter_val))\n",
    "        #print(f\"weighting_coverage: {weighting_coverage}\")\n",
    "        final_val = lum_factor*weighting_coverage*schechter_val\n",
    "        #print(f\"final value: {final_val}\")\n",
    "        return final_val\n",
    "    def denom_func(M):\n",
    "        lum_factor = 10**(-0.4*M)\n",
    "        schechter_val = schechter_model(M)\n",
    "        final_val = lum_factor*schechter_val\n",
    "\n",
    "        return final_val\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    numerator = quad(num_func, -28, -18)\n",
    "    denominator = quad(denom_func, -28, -18)\n",
    "\n",
    "    print(numerator)\n",
    "    print(denominator)\n",
    "\n",
    "    f_missed = numerator[0]/denominator[0]\n",
    "\n",
    "    print(f_missed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_f = hstack([data_f, Table({\"P_GAL\": P_gal.value})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_f[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Analyse the data: How much luminosity do we cover? Which galaxies should we observe?\n",
    "\n",
    "Lets look into what information we can retrieve from here. First we define, how many galaxies we could observe (N_galaxies).\n",
    "\n",
    "We then calculate the luminosity (lum) from all the galaxies in this area, i.e. the total luminosity. From there do some comparison between the covered luminosity and total luminosity depending on the ranking by probability:\n",
    "- simple \"Maximum lum that could be covered\" by looking at the brightest galaxies\n",
    "- Calculate the probability this way (done above) (from https://arxiv.org/pdf/1710.05452.pdf w/o normalization), both using a static and variable distance and error: $$P_{gal} = k^{-1}\\tilde{L_z}(1-f_{missed})\\cdot P_{2D}\\left(1-\\text{erf}\\left(\\frac{|D_{Gal}-D_{LVC}|}{\\sqrt{\\sigma_{D,gal}^{2}+\\sigma_{D,LVC}^{2}+\\sigma_{D,cosmology}^{2}}}\\right)\\right)$$\n",
    "- others may follow...\n",
    "\n",
    "**Wrong formula**\n",
    "\n",
    "Since we have the actual luminosity (here in r-band though), we don't need to again calculate the $$\\tilde{L}_{gal}$$ values. In the mentioned paper, they only use a fixed distance and error for the LIGO data. I will try both, to get an idea what the differences are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_galaxies = 200 # how many galaxies we can cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_lum = np.sum(data_f[\"LUM_R\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compare lum with luminosity ranking\n",
    "\n",
    "https://iopscience.iop.org/article/10.3847/0067-0049/226/1/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sort_by_lums = data_f.group_by(\"LUM_R\")\n",
    "data_sort_by_lums = data_sort_by_lums[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lum_after_N_highest = np.sum(data_sort_by_lums[\"LUM_R\"][:N_galaxies])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The theoretical maximum luminosity we could cover is {lum_after_N_highest}, i.e. {lum_after_N_highest*100/total_lum} % of the total luminosity\")\n",
    "print(\"The TARGETID values of these galaxies are: \\n\", data_sort_by_lums[\"TARGETID\"][:N_galaxies])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compare lum with P_Gal ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sort_by_PGal = data_f.group_by(\"P_GAL\")\n",
    "data_sort_by_PGal = data_sort_by_PGal[::-1]\n",
    "\n",
    "lum_after_N_PGal = np.sum(data_sort_by_PGal[\"LUM_R\"][:N_galaxies])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The luminosity we cover when observing {N_galaxies} galaxies ranked by P_gal is {lum_after_N_PGal}W, i.e. {lum_after_N_PGal*100/total_lum}% of the total (DESI) luminosity in the search area\") \n",
    "print(\"The TARGETID values of these galaxies are: \\n\", data_sort_by_PGal[\"TARGET_RA\", \"TARGET_DEC\", \"SERSIC\", \"TARGETID\", \"P_GAL\"][:N_galaxies])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_Gal_cumsum = np.cumsum(data_sort_by_PGal[\"P_GAL\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(range(len(P_Gal_cumsum))),  P_Gal_cumsum)\n",
    "plt.xscale(\"log\")\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "plt.xlim(1,len(P_Gal_cumsum))\n",
    "plt.savefig(dpi = 100, fname = f\"{superevent_name}PGAL_CUMULATIVE099.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"data_sort_by_PGal_C = data_f.group_by(\"P_GAL_C\")\n",
    "data_sort_by_PGal_C = data_sort_by_PGal_C[::-1]\n",
    "\n",
    "lum_after_N_PGal_C = np.sum(data_sort_by_PGal_C[\"LUM_R\"][:N_galaxies])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"print(f\"The maximum luminosity we cover when ranked by P_gal_C is {lum_after_N_PGal_C}, i.e. {lum_after_N_PGal_C*100/total_lum}% of the total luminosity\") \n",
    "print(\"The TARGETID values of these galaxies are: \\n\", data_sort_by_PGal_C[\"TARGETID\", \"P_GAL\"][:N_galaxies])\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exclude unobservabe objects from the PGal ranking and export list\n",
    "\n",
    "We want to cut out galaxies that are not observable by Wendelstein. For this we need the date (to calculate alt and az; we do this for three times to check observability) and Wendelstein location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert DEG:MIN:SEC to deg with decimals...\n",
    "def conv_to_deg(deg, arcmin, arcsec):\n",
    "    return (deg+ arcmin/60+ arcsec/3600)*u.deg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get the observable objects (i.e. alt > 0) in the next night and apply that selection to our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the timzone\n",
    "timezone = \"Europe/Berlin\"\n",
    "tz_Berlin = pytz.timezone(timezone)\n",
    "\n",
    "# get the date (and actually time, but that it useless for us) of today and tomorrow\n",
    "day_Berlin = datetime.now(tz_Berlin)\n",
    "tmrw_Berlin = day_Berlin + timedelta(hours = 23, minutes = 59, seconds = 59)\n",
    "\n",
    "# set the Wendelstein location\n",
    "Wendelstein_loc = EarthLocation(lat = conv_to_deg(47, 42, 13.1), lon = conv_to_deg(12, 0, 43.4), height = 1838*u.m) #check - or + 12\n",
    "\n",
    "# define the sun object, which will tell us sunrise and set\n",
    "sun = SunTimes(Wendelstein_loc.lon.to_value(), Wendelstein_loc.lat.to_value(), Wendelstein_loc.height.to_value())\n",
    "sunset_time = sun.setwhere(day_Berlin, timezone)\n",
    "sunrise_time = sun.risewhere(tmrw_Berlin, timezone)\n",
    "\n",
    "# get a time array that includes the sunset time and from there a 30min interval until sunset\n",
    "t = np.arange(sunset_time, sunrise_time, timedelta(minutes=45)).astype(datetime)              # this may throw a deprecation warning, but seems fine anyway: https://github.com/numpy/numpy/issues/23904\n",
    "\n",
    "# a little cheap but effective: observation at sunset is useless, start 1h after:\n",
    "t = t[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the SkyCoords for each objects in the data catalog\n",
    "target_SkyCoord = SkyCoord(data_sort_by_PGal[\"TARGET_RA\"], data_sort_by_PGal[\"TARGET_DEC\"], unit = \"deg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Alt for the different times during the night\n",
    "# t should be an array that consists of a couple of night times, so if *all* of these times don't have a positive alt it will be discarded\n",
    "target_Alt = [target_SkyCoord.transform_to(AltAz(obstime = time, location = Wendelstein_loc)).alt.value for time in t]\n",
    "\n",
    "# now turn it into a numpy array\n",
    "target_Alt = np.array(target_Alt)\n",
    "\n",
    "# swap the axes so that the first axis is all the different targets and the second then includes the alt per observing time\n",
    "target_Alt_swapped = np.swapaxes(target_Alt, 0,1)\n",
    "\n",
    "# now create the selection for each target in the P_Gal catalog\n",
    "sel = []\n",
    "for target in target_Alt_swapped:\n",
    "    sel.append((target > 0).any())\n",
    "    \n",
    "# now apply the selection to the dataset\n",
    "data_sort_by_PGal_observable = data_sort_by_PGal[sel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There were {len(data_sort_by_PGal) - len(data_sort_by_PGal_observable)} non-observable objects in the current dataset. \\nWe have {len(data_sort_by_PGal_observable)} objects remaining\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Export the final list & print some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"These are the stats for {superevent_name} follow up :\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_min_z = np.min(data_sort_by_PGal_observable[\"Z\"])\n",
    "data_max_z = np.max(data_sort_by_PGal_observable[\"Z\"])\n",
    "print(f\"Localization area: {area_99.to_value(u.deg**2)*u.deg**2} \\nRedshift range in data: {data_min_z} to {data_max_z} \\ncorresponding lum distance range: {data_min_dist} to {data_max_dist} \\nhighest prob location is at RA = {ra.deg*u.deg} and dec = {dec.deg*u.deg} \\ntotal No. of objects in desi data is {len(data_sort_by_PGal)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sort_by_PGal_observable[\"TARGET_RA\", \"TARGET_DEC\", \"SERSIC\", \"Z\", \"P_GAL\"][:N_galaxies].write(f'PGAL/PGAL_{superevent_name}.ecsv', delimiter=',', format='ascii', overwrite = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add timing data to table, "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MambaTest)",
   "language": "python",
   "name": "mambatest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
